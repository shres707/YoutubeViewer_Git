# -*- coding: utf-8 -*-
"""data_preprocess_eda.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_aUY4G3_bd2ew2MeOdWq919D0m4G0Lo8
"""

import streamlit as st
import re
import nltk
#nltk.download('stopwords')
nltk.download('wordnet') 
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_words(text):
        return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

# REFERENCE : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b
def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags 
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def preprocess_comment(df):
  #Replace text to lowercase
  df['Comment'] = df['Comment'].apply(lambda x: " ".join(x.lower() for x in x.split()))
  #Remove Punctuation
  df['Comment'] = df['Comment'].str.replace('[^\w\s]','')
  #Removing Emoji
  df['Comment'] = df['Comment'].apply(lambda x: remove_emoji(x))
  #Removing URL
  df['Comment'] = df['Comment'].apply(lambda x: remove_urls(x))
  #Removing Stopwords
  stop = stopwords.words('english')
  df['Comment'] = df['Comment'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
  
  #df["Comment"] = df["Comment"].apply(lambda x: lemmatize_words(x))
  st.write(df["Comment"])

def app():
   st.title("Data Preprocess")
   df_unclean=st.session_state.test
   preprocess_comment(df_unclean)